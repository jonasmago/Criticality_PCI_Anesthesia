{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mne.time_frequency import psd_array_multitaper, psd_array_welch\n",
    "from scipy.signal import hilbert\n",
    "from scipy.signal import welch\n",
    "import multiprocessing as mp\n",
    "from scipy.io import loadmat\n",
    "from scipy.io import savemat\n",
    "from scipy import signal\n",
    "from fooof import FOOOF\n",
    "import edgeofpy as eop\n",
    "from scipy import stats\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import powerlaw\n",
    "import argparse\n",
    "import pickle\n",
    "import mne.io\n",
    "import mne\n",
    "import os\n",
    "import glob\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzing Avlanches of sub0 day2 jhana\n",
      "Opening raw data file data/input/continuous/sub0-day2-jhana-raw.fif...\n",
      "    Range : 512 ... 323799 =      2.000 ...  1264.840 secs\n",
      "Ready.\n",
      "Reading 0 ... 323287  =      0.000 ...  1262.840 secs...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/bq/hl737msd54q59fzf7spf0r7h0000gn/T/ipykernel_26237/1401125018.py:104: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  cut = np.int(sig_length*fs)\n",
      "Assuming nested distributions\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzing Avlanches of sub0 day2 mindfulness\n",
      "Opening raw data file data/input/continuous/sub0-day2-mindfulness-raw.fif...\n",
      "    Range : 512 ... 294535 =      2.000 ...  1150.527 secs\n",
      "Ready.\n",
      "Reading 0 ... 294023  =      0.000 ...  1148.527 secs...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jonasmago/PhD_code_data/github/Criticality_PCI_Anesthesia/venv/lib/python3.8/site-packages/powerlaw.py:1615: RuntimeWarning: invalid value encountered in true_divide\n",
      "  CDF = CDF/norm\n",
      "'nan' in fit cumulative distribution values.\n",
      "Likely underflow or overflow error: the optimal fit for this distribution gives values that are so extreme that we lack the numerical precision to calculate them.\n",
      "Assuming nested distributions\n",
      "/Users/jonasmago/PhD_code_data/github/Criticality_PCI_Anesthesia/venv/lib/python3.8/site-packages/edgeofpy/avalanche.py:702: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  br = np.ma.masked_invalid(events[1:] / events[:-1]).mean()\n",
      "/var/folders/bq/hl737msd54q59fzf7spf0r7h0000gn/T/ipykernel_26237/1401125018.py:213: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  output_2 = output_2.append({'sub': sub, 'day': day, 'condition': condition}, ignore_index=True)\n",
      "/var/folders/bq/hl737msd54q59fzf7spf0r7h0000gn/T/ipykernel_26237/1401125018.py:104: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  cut = np.int(sig_length*fs)\n",
      "Assuming nested distributions\n",
      "/Users/jonasmago/PhD_code_data/github/Criticality_PCI_Anesthesia/venv/lib/python3.8/site-packages/powerlaw.py:1615: RuntimeWarning: invalid value encountered in true_divide\n",
      "  CDF = CDF/norm\n",
      "'nan' in fit cumulative distribution values.\n",
      "Likely underflow or overflow error: the optimal fit for this distribution gives values that are so extreme that we lack the numerical precision to calculate them.\n",
      "Assuming nested distributions\n",
      "/var/folders/bq/hl737msd54q59fzf7spf0r7h0000gn/T/ipykernel_26237/1401125018.py:213: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  output_2 = output_2.append({'sub': sub, 'day': day, 'condition': condition}, ignore_index=True)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "out_dir = 'data/output/AVC/'\n",
    "in_dir = 'data/input/continuous/'\n",
    "\n",
    "FIL_FREQ = (1, 40) # bandpass frequencies\n",
    "THRESH_TYPE = 'both' # Fosque22: 'both'\n",
    "# GAMMA_EXPONENT_RANGE = (0, 2) #NOT USED CURRENTLY\n",
    "# LATTICE_SEARCH_STEP = 0.1 #NOT USED CURRENTLY\n",
    "BIN_THRESHOLD = float(2)\n",
    "MAX_IEI = float(0.008)\n",
    "BRANCHING_RATIO_TIME_BIN = float(0.008)\n",
    "#\n",
    "#  The BIN_THRESHOLD is too high (detecting very few events).\n",
    "# The MAX_IEI is too strict, discarding short events.\n",
    "\n",
    "# output\n",
    "out = {'mean_iei':[],\n",
    "        'tau':[],\n",
    "        'tau_dist':[],\n",
    "        'tau_dist_TR':[],\n",
    "        'alpha':[],\n",
    "        'alpha_dist':[],\n",
    "        'alpha_dist_TR':[],\n",
    "        'third':[],\n",
    "        'dcc_cn':[],\n",
    "        'avl_br':[],\n",
    "        'br':[],\n",
    "        'rep_dissimilarity_avg':[],\n",
    "        'rep_size':[],\n",
    "        'fano':[],\n",
    "        'chi_test':[],\n",
    "        'chi_notest':[],\n",
    "        'sig_length':[],\n",
    "        'len_avls':[],\n",
    "        'data_mean':[],\n",
    "        'data_std':[]\n",
    "        }\n",
    "\n",
    "# make output directory\n",
    "if not os.path.exists(out_dir):\n",
    "    os.makedirs(out_dir)\n",
    "\n",
    "# load patient info and conditions\n",
    "# info = pd.read_csv(args.part_info,sep = ',', index_col=None)\n",
    "# P_IDS = info['ID']\n",
    "# Cond = info['Cond']\n",
    "# Drug = info['Drug']\n",
    "\n",
    "paths = glob.glob(in_dir+'*.fif')\n",
    "paths.sort()\n",
    "\n",
    "#loop over all conditions and particiants\n",
    "# for i, p_id in enumerate(P_IDS):\n",
    "for path_i, path in enumerate(paths):\n",
    "    sub = os.path.basename(path)[:4]\n",
    "    day = os.path.basename(path)[5:9]\n",
    "    condition = os.path.basename(path)[10:-8]\n",
    "\n",
    "    print(f\"Analyzing Avlanches of {sub} {day} {condition}\");\n",
    "\n",
    "    #################################\n",
    "    #          LOAD  DATA          #\n",
    "    #################################\n",
    "\n",
    "    \n",
    "    # data = loadmat(input_fname)\n",
    "    # data = data['dataOnlyGoodDataPoints']\n",
    "    # fs = 1450\n",
    "    # sig_length = min(data.shape[1]/fs , 300)\n",
    "    # nr_channels =  data.shape[0]\n",
    "\n",
    "    # # cut data and only use first 5 min or less\n",
    "    # cut = np.int(sig_length*fs)\n",
    "    # data = data[:,:cut]\n",
    "\n",
    "    # data_mean = np.mean(np.abs(data))\n",
    "    # data_std = np.std(data)\n",
    "\n",
    "    # data_filt = mne.filter.filter_data(data, sfreq=fs, l_freq=FIL_FREQ[0], h_freq=FIL_FREQ[1],verbose=False)\n",
    "\n",
    "    # events_by_chan = eop.binarized_events(data_filt, threshold=BIN_THRESHOLD,\n",
    "    #                             thresh_type=THRESH_TYPE, null_value=0)\n",
    "    # events_one_chan = np.sum(events_by_chan, axis=0)\n",
    "\n",
    "\n",
    "    raw = mne.io.read_raw_fif(path, preload=True)\n",
    "    raw.interpolate_bads(reset_bads=True)\n",
    "    #raw.pick_types(eeg=True, meg=False, stim=False, eog=False, ecg=False, emg=False, misc=False, exclude='bads')\n",
    "\n",
    "    data = raw.get_data()[:32,:]\n",
    "    fs= 256\n",
    "    sig_length = min(data.shape[1]/fs , 300)\n",
    "    nr_channels =  data.shape[0]\n",
    "\n",
    "    # cut data and only use first 5 min or less\n",
    "    cut = np.int(sig_length*fs)\n",
    "    data = data[:,:cut]\n",
    "\n",
    "    data_mean = np.mean(np.abs(data))\n",
    "    data_std = np.std(data)\n",
    "\n",
    "    data_filt = mne.filter.filter_data(data, sfreq=fs, l_freq=FIL_FREQ[0], h_freq=FIL_FREQ[1],verbose=False)\n",
    "\n",
    "    events_by_chan = eop.binarized_events(data_filt, threshold=BIN_THRESHOLD,\n",
    "                                thresh_type=THRESH_TYPE, null_value=0)\n",
    "    events_one_chan = np.sum(events_by_chan, axis=0)\n",
    "\n",
    "\n",
    "    #################################\n",
    "    #    Avalanches                 #\n",
    "    #################################\n",
    "\n",
    "    # Detect avalanches\n",
    "    #breakpoint()\n",
    "    avls, _, _, mean_iei = eop.detect_avalanches(events_by_chan, fs,\n",
    "                                                    max_iei=MAX_IEI,\n",
    "                                                    threshold=BIN_THRESHOLD,\n",
    "                                                    thresh_type=THRESH_TYPE)\n",
    "\n",
    "    sizes = [x['size'] for x in avls]\n",
    "    dur_bin = [x['dur_bin'] for x in avls]\n",
    "    dur_sec = [x['dur_sec'] for x in avls]\n",
    "    len_avls = len(avls)\n",
    "    # save Avalanches\n",
    "    avls_out = f'{out_dir}/AVC_bin_{BIN_THRESHOLD}_iei_{MAX_IEI}/'\n",
    "    os.makedirs(avls_out,exist_ok = True)\n",
    "    with open(f'{avls_out}Avalanches_{sub}_{day}_{condition}.p', 'wb') as f:\n",
    "            pickle.dump(avls, f)\n",
    "\n",
    "    #################################\n",
    "    #    TAU                 #\n",
    "    #################################\n",
    "    # Estimate fit and extract exponents with min and max of data\n",
    "\n",
    "    size_fit = eop.fit_powerlaw(sizes, xmin=1, discrete = True, xmax = None)\n",
    "    tau = size_fit['power_law_exp']\n",
    "    tau_dist = size_fit['best_fit']\n",
    "    tau_dist_TR = size_fit['T_R_sum']\n",
    "\n",
    "\n",
    "    #################################\n",
    "    #    ALPHA                     #\n",
    "    #################################\n",
    "\n",
    "    #dur_bin_fit = eop.fit_powerlaw(dur_bin, discrete = True)\n",
    "    #alpha_bin = dur_bin_fit['power_law_exp']\n",
    "\n",
    "    dur_fit = eop.fit_powerlaw(dur_sec, xmin='min', xmax = None, discrete = False)\n",
    "    alpha = dur_fit['power_law_exp']\n",
    "    alpha_dist = dur_fit['best_fit']\n",
    "    alpha_dist_TR = dur_fit['T_R_sum']\n",
    "\n",
    "\n",
    "    #################################\n",
    "    #    Third   and DCC            #\n",
    "    #################################\n",
    "\n",
    "    #third_bin = eop.fit_third_exponent(sizes, dur_bin, discrete= True)\n",
    "    third = eop.fit_third_exponent(sizes, dur_sec, discrete= False, method = 'pl')\n",
    "\n",
    "    #dcc_cn_bin = eop.dcc(tau, alpha_bin, third_bin)\n",
    "    dcc_cn = eop.dcc(tau, alpha, third)\n",
    "\n",
    "\n",
    "    #################################\n",
    "    #    REPERTPOIRE               #\n",
    "    #################################\n",
    "\n",
    "    # Estimate avalanche functional repertoire\n",
    "    repertoire = eop.avl_repertoire(avls)\n",
    "    # normalize the repertoire by signal length\n",
    "    rep_size = repertoire.shape[0]/sig_length\n",
    "\n",
    "    #rep_similarity_mat = eop.avl_pattern_similarity(repertoire, norm=True)\n",
    "    rep_similarity_mat = eop.avl_pattern_dissimilarity(repertoire, norm=True)\n",
    "    rep_dissimilarity_avg = np.mean(rep_similarity_mat)\n",
    "\n",
    "    #################################\n",
    "    #    Branching Ratio            #\n",
    "    #################################\n",
    "    # Calculate avalanche branching ratio\n",
    "    avl_br = eop.avl_branching_ratio(avls)\n",
    "    # Calculate branching ratio\n",
    "    br = eop.branching_ratio(events_one_chan, BRANCHING_RATIO_TIME_BIN, fs)\n",
    "\n",
    "\n",
    "    #################################\n",
    "    #   Susceptibility              #\n",
    "    #################################\n",
    "\n",
    "    # Calculate Fano factor\n",
    "    fano = eop.fano_factor(events_one_chan)\n",
    "\n",
    "    # Calculate susceptibility\n",
    "    chi_test, _ = eop.susceptibility(events_by_chan,test = True)\n",
    "    chi_notest, _ = eop.susceptibility(events_by_chan,test = False)\n",
    "\n",
    "    ## Save output\n",
    "    for name in out.keys():\n",
    "        out[name].append(locals()[name])\n",
    "    output_df = pd.DataFrame(out)\n",
    "\n",
    "    if path_i == 0:\n",
    "        output_2 = pd.DataFrame(columns=['sub', 'day', 'condition'])\n",
    "    output_2 = output_2.append({'sub': sub, 'day': day, 'condition': condition}, ignore_index=True)\n",
    "\n",
    "tosave = pd.concat((output_2,output_df),axis = 1)\n",
    "tosave.to_csv(f'{out_dir}/AVC_bin_{BIN_THRESHOLD}_iei_{MAX_IEI}.csv', index=False, sep=',')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## re load avalanches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of avalanches: 5488\n",
      "First avalanche example: {'start_time': 0.0078125, 'end_time': 0.01953125, 'size': 18, 'dur_sec': 0.01171875, 'dur_bin': 2, 'n_chan': 18, 'profile': array([17.,  1.]), 'pattern': array([0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1,\n",
      "       1, 0, 1, 1, 1, 1, 1, 0, 1, 1])}\n"
     ]
    }
   ],
   "source": [
    "avl_file = \"data/output/AVC/AVC_bin_2.0_iei_0.008/Avalanches_sub0_day2_jhana.p\"\n",
    "with open(avl_file, \"rb\") as f:\n",
    "    avalanches = pickle.load(f)\n",
    "\n",
    "# Check the structure of the loaded data\n",
    "print(f\"Number of avalanches: {len(avalanches)}\")\n",
    "print(\"First avalanche example:\", avalanches[0])  # Print the first avalanche event\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
